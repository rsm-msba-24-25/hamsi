---
title: "Poisson Regression Examples"
author: "Hamsavi Krishnan"
date: 05/07/2025
callout-appearance: minimal # this hides the blue "i" icon on .callout-notes
---


## Blueprinty Case Study

### Introduction

Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty's software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty's software and after using it. Unfortunately, such data is not available. 

However, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm's number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty's software. The marketing team would like to use this data to make the claim that firms using Blueprinty's software are more successful in getting their patent applications approved.

:::: {.callout-note collapse="true"}
### Data


```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false

library(tidyverse)

# Read in the Blueprinty dataset
blueprinty <- read_csv("C:/Users/krish/hamsavi/blog/project4/blueprinty.csv")
airbnb <- read_csv("C:/Users/krish/hamsavi/blog/project4/airbnb.csv")


```
::::


:::: {.callout-note collapse="true"}
### Patent Count Distributions by Customer Status



```{r}
#| code-fold: true
#| code-summary: "Show code"
#| fig-cap: "Comparison of patents awarded between Blueprinty customers and non-customers"
#| fig-width: 10
#| fig-height: 5
#| message: false
#| warning: false

library(ggplot2)
library(dplyr)
library(patchwork)

blueprinty <- blueprinty %>%
  mutate(iscustomer = factor(iscustomer, levels = c(0, 1), labels = c("Non-Customer", "Customer")))

hist_non <- ggplot(filter(blueprinty, iscustomer == "Non-Customer"),
                   aes(x = patents)) +
  geom_histogram(binwidth = 1, fill = "#90CAF9", color = "black") +
  labs(title = "Non-Customers", x = "Patents Awarded", y = "Count") +
  theme_minimal()

hist_cust <- ggplot(filter(blueprinty, iscustomer == "Customer"),
                    aes(x = patents)) +
  geom_histogram(binwidth = 1, fill = "#F48FB1", color = "black") +
  labs(title = "Customers", x = "Patents Awarded", y = "Count") +
  theme_minimal()

hist_non + hist_cust
```

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false

library(dplyr)
library(knitr)

blueprinty %>%
  group_by(iscustomer) %>%
  summarise(
    mean_patents = round(mean(patents), 2),
    n = n()
  ) %>%
  mutate(iscustomer = ifelse(iscustomer == 1, "Customer", "Non-Customer")) %>%
  kable(
    caption = "Mean number of patents awarded by Blueprinty customer status",
    col.names = c("Customer Status", "Mean Patents", "Sample Size")
  )
```
In this analysis, we compare the distribution of patents awarded between Blueprinty customers and non-customers. The histograms show that customers tend to have higher patent counts on average. The accompanying summary table confirms this, with customers averaging 4.13 patents versus 3.47 for non-customers. This suggests a potential association between using Blueprinty‚Äôs software and increased innovation output. However, because customer status is not randomly assigned, these differences may also be influenced by other factors like firm age or region. To account for these confounding variables, we proceed with a Poisson regression model that adjusts for such covariate


Firms that use Blueprinty's software tend to have a distribution shifted toward higher patent counts, suggesting a possible link between software use and innovation productivity. However, because customer status is not randomly assigned, these differences could reflect other factors like firm age or region, which we will adjust for in the Poisson regression.

::::

:::: {.callout-note collapse="true"}
### Demographic Differences Between Blueprinty Customers and Non-Customers
Blueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false
library(ggplot2)
library(dplyr)

# Rename your dataset to avoid conflict with built-in function
blueprinty <- read_csv("C:/Users/krish/hamsavi/blog/project4/blueprinty.csv")

# Remove missing customer values and label them
blueprinty <- blueprinty %>%
  filter(!is.na(iscustomer)) %>%
  mutate(iscustomer = factor(iscustomer, labels = c("Non-Customer", "Customer")))

# -----------------------------
# 1. Age Distribution Boxplot
# -----------------------------
ggplot(blueprinty, aes(x = iscustomer, y = age, fill = iscustomer)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Age Distribution by Customer Status",
    x = "Customer Status",
    y = "Age"
  ) +
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("lightblue", "lightgreen"))

# -----------------------------
# 2. Region Distribution Barplot
# -----------------------------
blueprinty %>%
  group_by(region, iscustomer) %>%
  summarise(count = n(), .groups = "drop") %>%
  group_by(iscustomer) %>%
  mutate(percentage = count / sum(count) * 100) %>%
  ggplot(aes(x = region, y = percentage, fill = iscustomer)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  labs(
    title = "Region Distribution by Customer Status",
    x = "Region",
    y = "Percentage (%)",
    fill = "Customer Status"
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("orange", "tomato")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

To investigate whether customer and non-customer firms differ demographically, we compare their age and regional distributions. The boxplot of Age Distribution by Customer Status shows that customers tend to be slightly older on average, although there is considerable overlap between the two groups. This age difference may partly explain the higher patent counts observed among customers earlier.

We also examine regional representation using a grouped bar chart. The plot indicates that Blueprinty customers are disproportionately concentrated in the Northeast, while non-customers are more evenly spread across other regions like the Midwest and South. These regional and age-based differences highlight the importance of controlling for such covariates when modeling patent outcomes ‚Äî otherwise, we risk attributing differences in patenting activity to software use when they may actually stem from underlying demographic variation
::::

:::: {.callout-note collapse="true"}
### Estimation of Simple Poisson Model

Since our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.



The probability mass function of the Poisson distribution is given by:

$$
f(Y_i \mid \lambda) = \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Assuming we have \( n \) independent observations \( Y_1, Y_2, \dots, Y_n \), the **likelihood function** is:

$$
L(\lambda) = \prod_{i=1}^{n} \frac{e^{-\lambda} \lambda^{Y_i}}{Y_i!}
$$

Taking the natural logarithm of the likelihood, the **log-likelihood function** becomes:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$

This function will be maximized in the next step to estimate the value of \( \lambda \) that best fits the data.

::::

:::: {.callout-note collapse="true"}
### Exploring the Poisson Likelihood Function


To estimate the parameter 
ùúÜ
Œª for a Poisson distribution, we begin by examining the shape of the log-likelihood function. The log-likelihood measures how well different values of 
ùúÜ
Œª explain the observed number of patents in our sample. The value that maximizes this function is the maximum likelihood estimate (MLE).

Below, we evaluate the log-likelihood across a range of plausible 
ùúÜ
Œª values and visualize the results. This approach gives us insight into how sensitive the model fit is to different assumptions about the average patent rate.

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| fig-cap: "Log-likelihood of the Poisson model across different lambda values"
#| fig-width: 6
#| fig-height: 5
#| message: false
#| warning: false

# Sample vector Y: observed patent counts
Y <- blueprinty$patents

# Define log-likelihood function for Poisson with constant lambda
loglik_poisson <- function(lambda, y) {
  if (lambda <= 0) return(-Inf)  # Poisson requires lambda > 0
  sum(dpois(y, lambda, log = TRUE))
}

# Generate a sequence of lambda values to test
lambda_vals <- seq(0.1, 10, by = 0.1)

# Compute log-likelihood for each lambda
loglik_vals <- sapply(lambda_vals, loglik_poisson, y = Y)

# Plot log-likelihood vs lambda
library(ggplot2)

ggplot(data.frame(lambda = lambda_vals, loglik = loglik_vals), aes(x = lambda, y = loglik)) +
  geom_line(color = "steelblue", size = 1) +
  geom_vline(xintercept = mean(Y), linetype = "dashed", color = "darkred") +
  labs(
    title = "Log-Likelihood for Poisson Model",
    x = expression(lambda),
    y = "Log-Likelihood"
  ) +
  theme_minimal()
```

The plot shows that the log-likelihood peaks around 
ùúÜ
=
mean
(
ùëå
)
Œª=mean(Y), which aligns with the known result that the MLE for a Poisson model with constant 
ùúÜ
Œª is simply the sample mean. This validates our understanding of how the Poisson distribution behaves and sets the stage for fitting a full model using optim() or glm() in the next step.

::::

:::: {.callout-note collapse="true"}
### Deriving the MLE Analytically



To deepen our understanding of the Poisson likelihood, we can derive the MLE analytically by taking the first derivative of the **log-likelihood function** and solving for \( \lambda \).

Recall the log-likelihood for \( n \) independent observations \( Y_1, Y_2, \dots, Y_n \sim \text{Poisson}(\lambda) \) is:

$$
\log L(\lambda) = \sum_{i=1}^{n} \left( -\lambda + Y_i \log \lambda - \log(Y_i!) \right)
$$

Taking the derivative with respect to \( \lambda \):

$$
\frac{d}{d\lambda} \log L(\lambda) = \sum_{i=1}^{n} \left( -1 + \frac{Y_i}{\lambda} \right)
= -n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i
$$

Setting this derivative equal to 0 and solving for \( \lambda \):

$$
-n + \frac{1}{\lambda} \sum_{i=1}^{n} Y_i = 0 \quad \Rightarrow \quad
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^{n} Y_i = \bar{Y}
$$

Thus, the **MLE of \( \lambda \)** is simply the **sample mean** \( \bar{Y} \). This result makes intuitive sense: the Poisson distribution models count data with a mean equal to \( \lambda \), so it's natural that the best estimate of \( \lambda \) is the average count in the data.
::::


:::: {.callout-note collapse="true"}
### Maximum Likelihood Estimation Using optim()




To estimate the value of 
ùúÜ
Œª that best explains our observed patent counts, we use Maximum Likelihood Estimation (MLE). The log-likelihood function we defined earlier is maximized using R‚Äôs optim() function. Since optim() minimizes by default, we provide the negative log-likelihood as the objective function.

```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false

# Define negative log-likelihood function
neg_loglik_poisson <- function(lambda, y) {
  if (lambda <= 0) return(Inf)  # invalid Œª
  -sum(dpois(y, lambda, log = TRUE))
}

# Run optimization using initial guess
mle_result <- optim(
  par = 1,  # initial guess for lambda
  fn = neg_loglik_poisson,
  y = blueprinty$patents,
  method = "Brent",  # 1D optimization method
  lower = 0.01, upper = 20
)

# Print estimated lambda
mle_result$par
```
#### What do you observe?

The optimizer returns a maximum likelihood estimate of:

$$
\hat{\lambda} \approx `r round(mle_result$par, 3)`
$$

This is very close to the **sample mean** of the observed number of patents. This confirms the theoretical result that, for a Poisson model with constant \( \lambda \), the MLE of \( \lambda \) is simply the sample mean:

$$
\hat{\lambda}_{\text{theory}} = \frac{1}{n} \sum_{i=1}^{n} Y_i
$$

::::

:::: {.callout-note collapse="true"}
### Estimation of Poisson Regression Model

Next, we extend our simple Poisson model to a Poisson Regression Model such that $Y_i = \text{Poisson}(\lambda_i)$ where $\lambda_i = \exp(X_i'\beta)$. The interpretation is that the success rate of patent awards is not constant across all firms ($\lambda$) but rather is a function of firm characteristics $X_i$. Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.


```{r}
#| code-fold: true
#| code-summary: "Show code"
#| message: false
#| warning: false

# Log-likelihood for Poisson Regression Model
poisson_regression_loglikelihood <- function(beta, Y, X) {
  # Convert beta to vector if needed
  beta <- as.numeric(beta)
  
  # Linear predictor: eta = X * beta
  eta <- X %*% beta
  
  # Inverse link function: lambda = exp(eta)
  lambda <- exp(eta)

  # Log-likelihood: sum over all observations
  loglikelihood <- sum(Y * log(lambda) - lambda - lgamma(Y + 1))
  
  # Return NEGATIVE log-likelihood for minimization via optim()
  return(-loglikelihood)
}
```

The Poisson Regression model assumes:

$$
Y_i \sim \text{Poisson}(\lambda_i), \quad \text{where} \quad \lambda_i = \exp(X_i^\top \beta)
$$

The corresponding log-likelihood function is:

$$
\log \mathcal{L}(\beta) = \sum_{i=1}^{n} \left[ Y_i \log(\lambda_i) - \lambda_i - \log(Y_i!) \right]
$$

Substituting \( \lambda_i = \exp(X_i^\top \beta) \), we get:

$$
\log \mathcal{L}(\beta) = \sum_{i=1}^{n} \left[ Y_i X_i^\top \beta - \exp(X_i^\top \beta) - \log(Y_i!) \right]
$$

**Explanation of Model Setup**

This section introduces the **Poisson regression model**, which is used to model count data‚Äîin this case, the number of patents awarded to firms. The model assumes that the expected count, \( \lambda_i \), is not constant across firms but instead depends on firm-specific characteristics such as age, age squared, region, and customer status. These features are captured in the design matrix \( X_i \), and their influence is modeled through the linear predictor \( X_i^\top \beta \).

To ensure that the predicted count \( \lambda_i \) is always positive (a requirement for count data), the model uses the exponential link function:

\[
\lambda_i = \exp(X_i^\top \beta)
\]

The log-likelihood function shown here is the objective we will maximize to estimate the model parameters \( \beta \). This function captures how likely the observed data are given a particular set of parameter values, and maximizing it gives us the most plausible coefficients under the Poisson model assumptions.

This setup forms the mathematical foundation for fitting the model using optimization routines like `optim()` in R.

::::
:::: {.callout-note collapse="true"}
### Poisson Regression Estimation, Validation, and Interpretation of Patent Award Drivers

```{r}
#| code-fold: true
#| code-summary: "Run full estimation"
#| message: false
#| warning: false

# Load libraries
library(tidyverse)
library(broom)
library(knitr)

# Step 1: Load and preprocess data
blueprinty <- read_csv("C:/Users/krish/hamsavi/blog/project4/blueprinty.csv") %>%
  mutate(
    age_sq = age^2,
    region = factor(region),
    customer = as.integer(iscustomer)  # Fix here: use iscustomer
  )

# Step 2: Create design matrix and response
X <- model.matrix(~ age + age_sq + region + customer, data = blueprinty)
Y <- blueprinty$patents
init_beta <- rep(0, ncol(X))  # Initial guess for optimization

# Step 3: Define Poisson log-likelihood function
poisson_regression_likelihood <- function(beta, Y, X) {
  eta <- X %*% beta
  lambda <- exp(eta)
  loglik <- sum(Y * log(lambda) - lambda - lgamma(Y + 1))
  return(-loglik)
}

# Step 4: Optimize
mle_result <- optim(
  par = init_beta,
  fn = poisson_regression_likelihood,
  Y = Y,
  X = X,
  hessian = TRUE,
  method = "BFGS"
)

# Step 5: Extract estimates and SEs
beta_hat <- mle_result$par
hessian <- mle_result$hessian

# Check Hessian validity
if (!is.null(hessian) && is.matrix(hessian)) {
  var_cov <- solve(hessian)
  se_hat <- sqrt(diag(var_cov))
} else {
  se_hat <- rep(NA, length(beta_hat))
  warning("Hessian is invalid; SEs not computed.")
}

# Step 6: Output results
results <- tibble(
  Term = colnames(X),
  Estimate = round(beta_hat, 4),
  `Std. Error` = round(se_hat, 4)
)

kable(results, caption = "Poisson Regression Coefficient Estimates and Standard Errors")

```


```{r}
#| code-fold: true
#| code-summary: "Show glm() regression results"
#| message: false
#| warning: false

# Load necessary libraries
library(broom)
library(knitr)

# Fit Poisson regression using glm()
glm_model <- glm(
  patents ~ age + I(age^2) + region + customer,
  data = blueprinty,
  family = poisson(link = "log")
)

# Tidy up results and round for presentation
glm_results <- tidy(glm_model) %>%
  mutate(
    Estimate = round(estimate, 4),
    `Std. Error` = round(std.error, 4),
    `z value` = round(statistic, 2),
    `p-value` = round(p.value, 4)
  ) %>%
  select(term, Estimate, `Std. Error`, `z value`, `p-value`)

# Output as a formatted table
kable(glm_results, caption = "Poisson Regression Results Using glm()")

```


## Interpretation of Poisson Regression Results

The Poisson regression estimates the expected number of patents awarded as a function of **firm age**, **region**, and whether the firm is a **Blueprinty customer**.

### üîπ Key Findings:

- **Customer (Estimate = 0.2076, p < 0.001)**  
  Being a Blueprinty customer is associated with a **significantly higher rate of patent awards**.  
  Interpreting the coefficient:  
  $$\exp(0.2076) \approx 1.23$$  
  Customers have about **23% more patents**, on average, than non-customers, holding other factors constant.

- **Age and Age¬≤ (Both p < 0.001)**  
  The relationship between firm age and patent success is **non-linear**:  
  - The **positive coefficient on age** suggests older firms initially file more patents.  
  - The **negative coefficient on age¬≤** implies **diminishing returns** with age ‚Äî a concave-down relationship.

- **Region Variables (All p > 0.05)**  
  None of the regional dummies are statistically significant. This suggests that **region is not a strong predictor** of patent success in this model.

- **Intercept (Estimate = -0.5089, p = 0.0055)**  
  Represents the log expected count of patents for a **non-customer firm** in the baseline region (e.g., Midwest), with age and age¬≤ = 0. Mostly useful as a baseline level.

::::
:::: {.callout-note collapse="true"}
### Conclusion 


To understand the practical effect of **Blueprinty's software** on patent success, we compute **predicted patent counts** under two scenarios:

- **X‚ÇÄ**: Each firm is treated as a **non-customer** (`customer = 0`)
- **X‚ÇÅ**: Each firm is treated as a **customer** (`customer = 1`)

Using the estimated Poisson regression model, we compute predicted values (`y_pred_0` and `y_pred_1`) and then take the **average difference**.

```{r}
#| code-fold: true
#| code-summary: "Estimate average effect of Blueprinty's software using counterfactuals"
#| message: false
#| warning: false
#| results: 'asis'

# Make sure glm_result exists (run glm before this!)
if (!exists("glm_result")) {
  glm_result <- glm(
    patents ~ age + I(age^2) + region + iscustomer,
    data = blueprinty,
    family = poisson(link = "log")
  )
}

# Step 1: Create counterfactual design matrices
X_0 <- model.matrix(~ age + I(age^2) + region + iscustomer,
                    data = blueprinty %>% mutate(iscustomer = 0))
X_1 <- model.matrix(~ age + I(age^2) + region + iscustomer,
                    data = blueprinty %>% mutate(iscustomer = 1))

# Step 2: Predict lambda (expected number of patents)
lambda_0 <- exp(X_0 %*% coef(glm_result))
lambda_1 <- exp(X_1 %*% coef(glm_result))

# Step 3: Compute average difference
average_effect <- mean(lambda_1 - lambda_0)

# Step 4: Display result
average_effect

```



### Interpretation

The analysis reveals that, on average, firms using **Blueprinty's software** are predicted to receive approximately **0.79 more patents** over a 5-year period compared to if they were not customers ‚Äî holding all other factors (such as age and region) constant. 

This suggests that Blueprinty‚Äôs product is associated with a **substantial and positive effect on patent success**. While this estimate supports the marketing team‚Äôs claims, it is important to note that the analysis is observational in nature and does not account for unmeasured confounders (e.g., firm size, innovation strategy), which could influence the result.

::::


:::: {.callout-note collapse="true"}
## AirBnB Case Study

### Introduction

AirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City.  The data include the following variables:

:::: {.callout-note collapse="true"}
### Variable Definitions

    - `id` = unique ID number for each unit
    - `last_scraped` = date when information scraped
    - `host_since` = date when host first listed the unit on Airbnb
    - `days` = `last_scraped` - `host_since` = number of days the unit has been listed
    - `room_type` = Entire home/apt., Private room, or Shared room
    - `bathrooms` = number of bathrooms
    - `bedrooms` = number of bedrooms
    - `price` = price per night (dollars)
    - `number_of_reviews` = number of reviews for the unit on Airbnb
    - `review_scores_cleanliness` = a cleanliness score from reviews (1-10)
    - `review_scores_location` = a "quality of location" score from reviews (1-10)
    - `review_scores_value` = a "quality of value" score from reviews (1-10)
    - `instant_bookable` = "t" if instantly bookable, "f" if not

::::




:::: {.callout-note collapse="true"}
### Exploratory Data Analysis & Cleaning

```{r}
#| code-fold: true
#| code-summary: "Load and explore AirBnB dataset"
#| message: false
#| warning: false

library(tidyverse)
library(knitr)

# Load data
airbnb <- read_csv("airbnb.csv")

# Select relevant variables and drop rows with NAs
airbnb_clean <- airbnb %>%
  select(number_of_reviews, room_type, price, bathrooms, bedrooms,
         review_scores_cleanliness, review_scores_location, review_scores_value,
         instant_bookable, days) %>%
  drop_na()

# Nicely formatted summary statistics for number_of_reviews
airbnb_clean %>%
  summarise(
    Min = min(number_of_reviews),
    Q1 = quantile(number_of_reviews, 0.25),
    Median = median(number_of_reviews),
    Mean = mean(number_of_reviews),
    Q3 = quantile(number_of_reviews, 0.75),
    Max = max(number_of_reviews)
  ) %>%
  kable(caption = "Summary Statistics: Number of Reviews")

# Nicely formatted table of room types
airbnb_clean %>%
  count(room_type, name = "Count") %>%
  kable(caption = "Room Type Frequency Table")

```
```{r}
#| code-fold: true
#| code-summary: "Plot EDA graphs"
#| fig-cap: "Distribution of Number of Reviews"
#| fig-width: 6
#| fig-height: 4
#| message: false
#| warning: false

# Histogram of number_of_reviews
ggplot(airbnb_clean, aes(x = number_of_reviews)) +
  geom_histogram(fill = "#2c7fb8", color = "white", bins = 50) +
  labs(title = "Distribution of Number of Reviews",
       x = "Number of Reviews", y = "Frequency") +
  theme_minimal()
```
```{r}
#| code-fold: true
#| code-summary: "Boxplot by Room Type"
#| fig-cap: "Number of Reviews by Room Type"
#| fig-width: 6
#| fig-height: 4
#| message: false
#| warning: false

# Boxplot of reviews by room_type
ggplot(airbnb_clean, aes(x = room_type, y = number_of_reviews, fill = room_type)) +
  geom_boxplot() +
  labs(title = "Number of Reviews by Room Type",
       x = "Room Type", y = "Number of Reviews") +
  theme_minimal() +
  theme(legend.position = "none")
```

::::
:::: {.callout-note collapse="true"}
### Poisson Regression Model
```{r}
#| code-fold: true
#| code-summary: "Fit and display Poisson regression results"
#| message: false
#| warning: false

library(broom)
library(knitr)

# Fit Poisson regression model
poisson_model <- glm(
  number_of_reviews ~ room_type + price + bathrooms + bedrooms + 
    review_scores_cleanliness + review_scores_location + 
    review_scores_value + instant_bookable + days,
  data = airbnb_clean,
  family = poisson(link = "log")
)

# Tidy the summary and format
tidy(poisson_model) %>%
  mutate(
    Estimate = round(estimate, 4),
    `Std. Error` = round(std.error, 4),
    `z value` = round(statistic, 2),
    `p-value` = round(p.value, 4)
  ) %>%
  select(term, Estimate, `Std. Error`, `z value`, `p-value`) %>%
  kable(caption = "Poisson Regression Results: Number of Reviews")

```
::::

### Interpretation 

The model reveals significant variation in review counts based on listing characteristics:

- **Room Type**: Compared to an entire home, private and shared rooms tend to have different review patterns. Coefficients can be exponentiated to interpret as multiplicative changes.
- **Price**: A higher price is associated with a **lower number of reviews**, likely reflecting reduced affordability.
- **Review Scores**: Cleanliness, location, and value ratings are **positively associated** with bookings ‚Äî cleaner, well-located, and good-value properties tend to get more reviews.
- **Instant Bookable**: Listings marked as instantly bookable receive more reviews on average.
- **Days Listed**: The longer a property has been on the platform, the more reviews it accumulates ‚Äî as expected.

Overall, the Poisson model helps quantify how various features influence demand (proxied by review counts) on Airbnb.

::::



